{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSE ST451: Bayesian Machine Learning\n",
    "## Author: Kostas Kalogeropoulos\n",
    "\n",
    "## Week 3: Baysian Linear Regression part II\n",
    "\n",
    "Topics covered \n",
    " - Creating your own function in Python\n",
    " - Performing matrix operations\n",
    " - Conducting full MLE analysis, with confidence intervals rather than just point estimates for the regression coefficients\n",
    " - Fitting Bayesian Linear Regression models and summarising the posterior of the regressions coefficients\n",
    " - Calculate the marginal likelihood / model evidence for linear regression models to perform Bayesian model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with loading the necessary libraries. We will pretty much the same libraries as next week with three additions: **scipy** for matrix operations, **seaborn** for better looking plots and **statsmodels** for full linear regression output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np             \n",
    "import pandas as pd  #Python Data Analysis Library handle data in a user friendly way\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt #for plots\n",
    "import seaborn as sns # for better plots\n",
    "%matplotlib inline\n",
    "from sklearn import linear_model # A very popular Python library for Machine Learning\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split #needed to for assessing prediction\n",
    "from sklearn import datasets ## imports datasets from scikit-learn\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin with the polynomial example of last week. We will sumlate 20 points this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polynomial fitting exercises\n",
    "np.random.seed(1)\n",
    "n = 20 #number of points\n",
    "x = np.linspace(0, 1, n)\n",
    "f = np.sin(2*np.pi*x)\n",
    "y = f+0.3*np.random.randn(n)\n",
    "#plt.plot(xg,f,label='true process')\n",
    "plt.plot(x,y,'o',label='data')\n",
    "plt.plot(x,f,label='true process')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put x,y into the pandas dataframe called data. also put a vector of ones as x0\n",
    "x0 = np.ones(n)\n",
    "data = pd.DataFrame(np.column_stack([y,x0,x]),columns=['y','x0','x']) \n",
    "#Expand the data including powers of x up to 10\n",
    "for i in range(2,11):  #executes the following indented commands for i varying from 3 to 9\n",
    "    colname = 'x%d'%i # the %d %i puts a different number in the name of each variable\n",
    "    data[colname] = data['x']**i #raise to the power of i\n",
    "    # the for loop continues until the first time a command is not indented.\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we added x0 a vector of ones for the constant in the regression equation. This will be handy for the matrix operations.\n",
    "\n",
    "It doesn't affect what we were doing last time though. The code below gives us the same output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npower = 2\n",
    "predictors = ['x']\n",
    "predictors.extend(['x%d'%i for i in range(2,npower+1)])\n",
    "linreg = LinearRegression(fit_intercept=True,normalize=False)\n",
    "linreg.fit(data[predictors],data['y'])\n",
    "y_pred = linreg.predict(data[predictors])\n",
    "plt.plot(data['x'],y_pred, label='OLS fit')\n",
    "plt.plot(data['x'],data['y'],'o',label='data')\n",
    "plt.plot(x,f,label='true process')\n",
    "plt.title('Plot for power: %d'%npower)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix operations\n",
    "\n",
    "Below we find the MLE without the scikit linear regression function. For that we\n",
    "\n",
    "1. Form the design matrix $X$ and the response vector y\n",
    "2. Calculate the MLE as $$\\hat{\\beta} = (X^T X)^{-1}X^T y$$\n",
    "\n",
    "The code below does that and also prints $X^T X$ and $(X^T X)^{-1}$ for checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npower = 2\n",
    "predictors = ['x0','x']\n",
    "predictors.extend(['x%d'%i for i in range(2,npower+1)])\n",
    "X = data[predictors]\n",
    "y = data['y']\n",
    "XtX = X.T.dot(X) #X.T give X transpose X.dot(b) does matrix multiplication X*b\n",
    "XtX_inv = sc.linalg.inv(XtX) #sc.linalg.inv(A) provide the inverse of A\n",
    "beta = XtX_inv.dot(X.T.dot(y))\n",
    "XtX, XtX_inv, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the MLE found previously with the scikit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression(fit_intercept=True,normalize=False)\n",
    "predictors = ['x','x2']\n",
    "linreg.fit(data[predictors],data['y'])\n",
    "beta = [linreg.intercept_]\n",
    "beta.extend(linreg.coef_)\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go beyond the point estimate and calculate the variance of $\\hat{\\beta}$.\n",
    "\n",
    "The code below provides $\\hat{\\sigma}^2$, the covariance matrix $$\\mbox{var}[\\hat{\\beta}] = \\hat{\\sigma}^2 (X^T X)^{-1}, $$\n",
    "and its diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,p = X.shape\n",
    "e = y - X.dot(beta)\n",
    "sigma2_hat = e.T.dot(e)/(n-p)\n",
    "var_beta = sigma2_hat*(XtX_inv)\n",
    "sigma2_hat, var_beta, np.diag(var_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above quantities we can calculate confidence intervals for MLE's. For those we will need the 97.5\\% point of the $t_{n-p}$ distribution. \n",
    "\n",
    "The following code does that and also puts the results in pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate 95% CI for beta and present output via a pandas data frame \n",
    "tval = sc.stats.t.ppf(0.975,n-p)\n",
    "std_beta = np.sqrt(np.diag(var_beta))\n",
    "lower95 = beta - tval*std_beta\n",
    "upper95 = beta + tval*std_beta\n",
    "results = np.column_stack([beta,std_beta,lower95,upper95])\n",
    "col = ['coefficient','se','lower 95% bound','upper 95% bound']\n",
    "ind = ['intercept','x','x2']\n",
    "results = pd.DataFrame(results,columns = col,index=ind)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check our answers we also use the relevant function (.OLS) of the library **statsmodels**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['x','x2']\n",
    "X = data[predictors]\n",
    "X = sm.add_constant(X) \n",
    "model = sm.OLS(y, X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1\n",
    "\n",
    "1. Obtain the MLEs and 95\\% confidence intervals for the 3rd order polynomial without using any Python built in function. \n",
    "2. Check your answers against the statsmodels corresponding functions (.OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing functions in Python\n",
    "\n",
    "We will now put all the previous talk into a function. In other words we will create our own function rather than using the stasmodels/scikit learn ones. \n",
    "\n",
    "The inputs of the function will be $X$, $y$ and the names of the predictors. The output will be a data frame with the results (MLE's, se's and 95\\% CI's)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to get MLE of beta, its SE, 95%CIs and present the ouput  \n",
    "def LRST451(X,y,predictor_names):\n",
    "    col_names = ['beta','se','lower 95% bound','upper 95% bound']\n",
    "    XtX = X.T.dot(X)\n",
    "    XtX_inv = sc.linalg.inv(X.T.dot(X))\n",
    "    beta = XtX_inv.dot(X.T.dot(y))\n",
    "    e = y - X.dot(beta)\n",
    "    n,p = X.shape\n",
    "    sigma2_hat = e.T.dot(e)/(n-p)\n",
    "    var_beta = sigma2_hat*(XtX_inv)\n",
    "    tval = sc.stats.t.ppf(0.975,n-p)\n",
    "    se_beta = np.sqrt(np.diag(var_beta))\n",
    "    lower95 = beta - sc.stats.t.ppf(0.975,n-p)*se_beta\n",
    "    upper95 = beta + sc.stats.t.ppf(0.975,n-p)*se_beta\n",
    "    results = np.column_stack([beta,se_beta,lower95,upper95])\n",
    "    results = pd.DataFrame(results,columns = col_names,index=predictor_names)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is it, the function is now stored in Python's memory. \n",
    "\n",
    "From now on, we will only need to specify X,y predictor_names, and just write 'LRST451(X,y,predictor_names)'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up model (X) and y\n",
    "npower = 4\n",
    "predictors = ['x0','x']\n",
    "predictors.extend(['x%d'%i for i in range(2,npower+1)])\n",
    "X = data[predictors]\n",
    "y = data['y']\n",
    "results = LRST451(X,y,predictors)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Linear Regression\n",
    "\n",
    "We now turn to Bayesian inference. As before we will illustrate and then create a function that provides summaries of the posterior (Bayes estimates of $\\beta$ and 95\\% Credible Intervals).\n",
    "\n",
    "We start by calculating the posterior parameters that correspond to the unit information prior.\n",
    "\n",
    "Given $\\sigma^2$ and a **prior** of N($\\mu_0,\\sigma^2\\Omega_0)$, the **posterior** $\\pi(\\beta|X,y,\\sigma^2)$ is N($\\mu_n,\\sigma^2\\Omega_n^2$) where\n",
    "\n",
    "$$\\mu_n = (X^T X+\\Omega_0^{-1})^{-1} (\\Omega_0^{-1}\\mu_0 +X^{T}y)$$\n",
    "$$\\Omega_n =(X^T X +\\Omega_0^{-1})^{-1}$$\n",
    "\n",
    "To assign the unit information prior we set $\\Omega_0 = n(X^T X)^{-1}$ or else $\\Omega_0^{-1}=X^T X / n$.\n",
    "\n",
    "For $\\sigma^2$ if we set the **prior** IGamma$(a_0,b_0)$ we get the **posterior** IGamma$(a_n,b_n)$ where\n",
    "$$\n",
    "a_n = a_0+\\frac{n}{2}\n",
    "$$\n",
    "$$\n",
    "b_n = b_0+\\frac{1}{2} (y^T y + \\mu_0^T\\Omega_0^{-1}\\mu_0 +\\mu_n^T\\Omega_n^{-1}\\mu_n).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up X and y\n",
    "npower = 3\n",
    "predictors = ['x0','x']\n",
    "predictors.extend(['x%d'%i for i in range(2,npower+1)])\n",
    "X = data[predictors]\n",
    "y = data['y']\n",
    "n,p = X.shape\n",
    "\n",
    "#set up prior parameters\n",
    "mu_0 = np.zeros(p)\n",
    "XtX = X.T.dot(X)\n",
    "Om_0_inv = XtX/n #unit information prior\n",
    "a_0 = 0.01\n",
    "b_0 = 0.01\n",
    "\n",
    "#calculate posterior parameters\n",
    "Om_n_inv = XtX + Om_0_inv\n",
    "Om_n = sc.linalg.inv(Om_n_inv)\n",
    "term1 = Om_0_inv.dot(mu_0)+X.T.dot(y)\n",
    "mu_n = Om_n.dot(term1)\n",
    "a_n = a_0 + n/2\n",
    "term2 = y.T.dot(y)+mu_0.dot(Om_0_inv.dot(mu_0))-mu_n.dot(Om_n_inv.dot(mu_n))\n",
    "b_n = b_0 + term2/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo for credible intervals\n",
    "\n",
    "To obtain credible intervals for $\\beta$ we could use the $t$ distribution. But we would use Monte Carlo instead as this will cover more general models. e.g. logistic regression.\n",
    "\n",
    "So we will sample $N$ Monte Carlo samples from $\\pi(\\beta|y)$ and use them  for Monte Carlo inference (credible intervals, density plots etc)\n",
    "\n",
    "Monte Carlo Samples can be drawn by\n",
    "\n",
    "1. Generating samples $\\sigma_i^2$ from IGamma$(\\alpha_n,\\beta_n)$, $i=1,\\dots,N$,\n",
    "2. Draw $\\beta_i$ sample based on each $\\sigma_i^2$ from N($\\mu_n,\\sigma^2\\Omega_n^2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate 95% credible intervals\n",
    "N = 10000 #number of Monte Carlo samples\n",
    "sigma2 = 1/np.random.gamma(a_n, 1/b_n, N)\n",
    "betas = np.zeros((N,p))\n",
    "# draw N samples from the marginal posterior of beta\n",
    "for i in range(0,N):\n",
    "    s2 =sigma2[i]\n",
    "    cov = s2*Om_n\n",
    "    betas[i,] = np.random.multivariate_normal(mu_n,cov,1)\n",
    "\n",
    "#find the mean of each column which corresponds to each beta coefficient     \n",
    "mu_beta = np.mean(betas, axis=0)\n",
    "#find the 2.5 and 97.5 percentils which correspond to each beta coefficient \n",
    "lower95 = np.percentile(betas,2.5,axis=0)\n",
    "upper95 = np.percentile(betas,97.5,axis=0)\n",
    "mu_beta, lower95, upper95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we put it all in a function. This is quite handy as it is hard to find such a function in Python!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for Bayesian Linear regression, return posterior mean and 95% CIs\n",
    "def BLR(X,y,mu_0,Om_0_inv,a_0,b_0,ind_names,N):\n",
    "    col_names = ['posterior mean','lower 95% bound','upper 95% bound']\n",
    "    n,p = X.shape\n",
    "    XtX = X.T.dot(X)\n",
    "    Om_n_inv = XtX + Om_0_inv\n",
    "    Om_n = sc.linalg.inv(Om_n_inv)\n",
    "    term1 = Om_0_inv.dot(mu_0)+X.T.dot(y)\n",
    "    mu_n = Om_n.dot(term1)\n",
    "    a_n = a_0 + n/2\n",
    "    term2 = y.T.dot(y)+mu_0.dot(Om_0_inv.dot(mu_0))-mu_n.dot(Om_n_inv.dot(mu_n))\n",
    "    b_n = b_0 + term2/2\n",
    "    sigma2 = 1/np.random.gamma(a_n, 1/b_n, N)\n",
    "    betas = np.zeros((N,p))\n",
    "    # draw N samples from the marginal posterior of beta\n",
    "    for i in range(0,N):\n",
    "        s2 =sigma2[i]\n",
    "        cov = s2*Om_n\n",
    "        betas[i,] = np.random.multivariate_normal(mu_n,cov,1)\n",
    "\n",
    "    #find the mean of each column which corresponds to each beta coefficient     \n",
    "    mu_beta = np.mean(betas, axis=0)\n",
    "    #find the 2.5 and 97.5 percentils which correspond to each beta coefficient \n",
    "    lower95 = np.percentile(betas,2.5,axis=0)\n",
    "    upper95 = np.percentile(betas,97.5,axis=0)\n",
    "    results = np.column_stack([mu_beta,lower95,upper95])\n",
    "    results = pd.DataFrame(results,columns = col_names,index=ind_names)\n",
    "    return results, mu_n, Om_n, a_n, b_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only need to spefcify the data and the prior parameters and input them to the BLR function. \n",
    "\n",
    "We will get back a pandas dataframe with the results.\n",
    "\n",
    "This works for any power from 1 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npower = 2\n",
    "predictors = ['x0','x']\n",
    "predictors.extend(['x%d'%i for i in range(2,npower+1)])\n",
    "X = data[predictors]\n",
    "y = data['y']\n",
    "n,p = X.shape\n",
    "\n",
    "#set up prior parameters\n",
    "mu_0 = np.zeros(p)\n",
    "XtX = X.T.dot(X)\n",
    "Om_0_inv = XtX/n #unit information prior\n",
    "a_0 = 0.01\n",
    "b_0 = 0.01\n",
    "\n",
    "N=10000 #Monte Carlo sample size\n",
    "\n",
    "results, mu_n, Om_n, a_n, b_n = BLR(X,y,mu_0,Om_0_inv,a_0,b_0,predictors,N)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal likelihood / Model Evidence\n",
    "\n",
    "Finally we will calculate the marginal likelihood / model evidence of a given model. We will use the following trick covered in the lecture\n",
    "\n",
    "We can write\n",
    "$$\n",
    "\\pi(\\beta,\\sigma^2|y,X)=\\frac{\\pi(y|\\beta,\\sigma^2,X)\\pi(\\beta,\\sigma^2)}{\\pi(y|X)},\n",
    "$$\n",
    "or else\n",
    "$$\n",
    "\\pi(y|X)=\\frac{\\pi(y|\\beta,\\sigma^2,X)\\pi(\\beta,\\sigma^2)}{\\pi(\\beta,\\sigma^2|y,X)},\n",
    "$$\n",
    "for all $\\beta, \\sigma^2$.\n",
    "\n",
    "The expression above contains known Normal and Inverse Gamma pdfs so we can just evaluate for -say- the posterior mean of $\\beta$, $\\sigma^2$.\n",
    "\n",
    "The calculation involves the following steps\n",
    "\n",
    "1. Specify data and prior\n",
    "2. get the parameters of the posteriod distribution\n",
    "3. evaluate the log-likelihood, log-prior and log-posterior at the posterior mean\n",
    "4. The log-evidence is equal to log-likelihood + log-prior - log-posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "npower = 3\n",
    "predictors = ['x0','x']\n",
    "predictors.extend(['x%d'%i for i in range(2,npower+1)])\n",
    "X = data[predictors]\n",
    "y = data['y']\n",
    "n,p = X.shape\n",
    "\n",
    "# Step 2\n",
    "mu_0 = np.zeros(p)\n",
    "XtX = X.T.dot(X)\n",
    "Om_0_inv = XtX/n #unit information prior\n",
    "a_0 = 0.1\n",
    "b_0 = 0.1\n",
    "N=10000 #Monte Carlo sample size\n",
    "results, mu_n, Om_n, a_n, b_n = BLR(X,y,mu_0,Om_0_inv,a_0,b_0,predictors,N)\n",
    "\n",
    "# Step 3\n",
    "# evaluate log-likelihood\n",
    "beta = mu_n\n",
    "sigma2 = b_n/(a_n-1)\n",
    "Om_0 = sc.linalg.inv(Om_0_inv)\n",
    "mu = X.dot(beta)\n",
    "cov = sigma2*np.eye(n)\n",
    "ll = sc.stats.multivariate_normal.logpdf(y,mu,cov)\n",
    "# evaluate log-prior\n",
    "lprior = a_0*np.log(b_0)-sc.special.loggamma(a_0)- (a_0+1)*np.log(sigma2)-b_0/sigma2\n",
    "lprior = lprior + sc.stats.multivariate_normal.logpdf(beta,mu_0,sigma2*Om_0)\n",
    "# evaluate log-posterior\n",
    "lpost = a_n*np.log(b_n)-sc.special.loggamma(a_n)- (a_n+1)*np.log(sigma2)-b_n/sigma2\n",
    "lpost = lpost + sc.stats.multivariate_normal.logpdf(beta,mu_n,sigma2*Om_n)\n",
    "\n",
    "lmodevid = ll+lprior-lpost\n",
    "lmodevid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make it a function. Again not easy to find such a function, keep it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModEvidence(X,y,mu_n,Om_n,mu_0,Om_0_inv,a_n,b_n,a_0,b_0): \n",
    "    beta = mu_n\n",
    "    sigma2 = b_n/(a_n-1)\n",
    "    Om_0 = sc.linalg.inv(Om_0_inv)\n",
    "    mu = X.dot(beta)\n",
    "    cov = sigma2*np.eye(X.shape[0])\n",
    "    ll = sc.stats.multivariate_normal.logpdf(y,mu,cov)\n",
    "    # evaluate log-prior\n",
    "    lprior = a_0*np.log(b_0)-sc.special.loggamma(a_0)- (a_0+1)*np.log(sigma2)-b_0/sigma2\n",
    "    lprior = lprior + sc.stats.multivariate_normal.logpdf(beta,mu_0,sigma2*Om_0)\n",
    "    # evaluate log-posterior\n",
    "    lpost = a_n*np.log(b_n)-sc.special.loggamma(a_n)- (a_n+1)*np.log(sigma2)-b_n/sigma2\n",
    "    lpost = lpost + sc.stats.multivariate_normal.logpdf(beta,mu_n,sigma2*Om_n)\n",
    "    lmodevid = ll+lprior-lpost\n",
    "    return lmodevid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get the evidence for a model we can simply use the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "npower = 3\n",
    "predictors = ['x0','x']\n",
    "predictors.extend(['x%d'%i for i in range(2,npower+1)])\n",
    "X = data[predictors]\n",
    "y = data['y']\n",
    "n,p = X.shape\n",
    "\n",
    "# Prior\n",
    "mu_0 = np.zeros(p)\n",
    "XtX = X.T.dot(X)\n",
    "Om_0_inv = XtX/n #unit information prior\n",
    "a_0 = 0.1\n",
    "b_0 = 0.1\n",
    "\n",
    "# Calculation\n",
    "N=10000 #Monte Carlo sample size\n",
    "results, mu_n, Om_n, a_n, b_n = BLR(X,y,mu_0,Om_0_inv,a_0,b_0,predictors,N)\n",
    "lmodevid = ModEvidence(X,y,mu_n,Om_n,mu_0,Om_0_inv,a_n,b_n,a_0,b_0)\n",
    "lmodevid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2\n",
    "\n",
    "Find the order of polynomial with the highest model evidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autmobile Bodily Injury Claims Data\n",
    "\n",
    "Source: Insurance Research Council (IRC)\n",
    "\n",
    "The data are automobile injury claims data using data from the Insurance Research Council (IRC), a division of the American Institute for Chartered Property Casualty Underwriters and the Insurance Institute of America. The data, collected in 2002, contains information on the gender of the claimant, attorney involvement, years of driving experience and the economic loss (LOSS, in thousands). A detailed description of the variables in the data is provided below:\n",
    "\n",
    "- Attorney: Whether the claimant is represented by an attorney (=1 if yes and =0 if no)\n",
    "- CLMSEX: Claimant’s gender (=1 if male and =0 if female \n",
    "- CLMAGE: Claimant’s age minus the age driving license was obtained\n",
    "- LOSS: The claimant’s total economic loss (in thousands $)\n",
    "\n",
    "For confidentiality issues we consider here a sample of  simulated data similar to that of a state is considered. \n",
    "\n",
    "The data are in the provided file 'automobileBI.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto = pd.read_csv(\"automobileBI.csv\") \n",
    "Auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(Auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3\n",
    "\n",
    "Consider all possible regression models with the LOSS variable as response and some (or all or none) of the other variables present (in their current form, no polynomials). There are 8 such models.\n",
    "\n",
    "Calculate the model evidence for each of them, report the optimal one and present posterior estimates of its coefficients.\n",
    "\n",
    "What can we say about information on gender?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
