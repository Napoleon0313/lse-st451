{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSE ST451: Bayesian Machine Learning\n",
    "## Author: Kostas Kalogeropoulos\n",
    "\n",
    "## Week 2: Baysian Linear Regression part I\n",
    "\n",
    "Topics covered \n",
    " - Working with Pandas data frames\n",
    " - Working with 'for' loops in Python\n",
    " - Fitting linear regression models\n",
    " - Polynomial curve fitting\n",
    " - Introduction to training and test error concepts\n",
    " - Ridge regression\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with loading the necessary libraries. We will use **numpy** and **matplotlib** that we saw on week 1 and gain familiarity with **pandas** for handling data. We will also get started on **sklearn** for linear and ridge regression, spliting data into train and test samples and get our hands on a real dataset on Boston house prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np             \n",
    "import pandas as pd           #Python Data Analysis Library handle data in a user friendly way\n",
    "#import random\n",
    "import matplotlib.pyplot as plt #for plots\n",
    "%matplotlib inline\n",
    "from sklearn import linear_model # A very popular Python library for Machine Learning\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split #needed to for assessing prediction\n",
    "from sklearn import datasets ## imports datasets from scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression / Polynomial Curve fitting\n",
    "\n",
    "We start with the polynomial curve fitting exercise seen in the lecture slides. The *truth* is the functions $sin(2\\pi x)$ for $x \\in [0,1]$. We observe 10 points, equispaced in $[0,1]$ - array x,  with $N(0,0.3^2)$ independent measurement error, array y. The array xg contains a grid of 100 points in $[0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polynomial fitting exercise\n",
    "np.random.seed(1)\n",
    "x = np.linspace(0, 1, 10)\n",
    "xg = np.linspace(0,1,100)\n",
    "f = np.sin(2*np.pi*xg)\n",
    "y = np.sin(2*np.pi*x)+0.3*np.random.randn(10)\n",
    "plt.plot(xg,f,label='true process')\n",
    "plt.plot(x,y,'o',label='data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to start working with pandas data frames we put x and y in pandas data frame called 'data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put x,y into the pandas dataframe called data \n",
    "data = pd.DataFrame(np.column_stack([y,x]),columns=['y','x']) #columns, then column names\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add the variable 'x2' containing the values of x$^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['x2']=data['x']**2\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to also add the variable x$^3$ to x$^9$ so that we fit polynomials up to the 9-th degree. This can be done very quickly using a 'for loop'. See below how to use a 'for loop' in an example were string and numerical variables are being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expand the data including powers of x up to 9\n",
    "for i in range(3,10):  #executes the following indented commands for i varying from 3 to 9\n",
    "    colname = 'x%d'%i # the %d %i puts a different number in the name of each variable\n",
    "    data[colname] = data['x']**i #raise to the power of i\n",
    "    # the for loop continues until the first time a command is not indented.\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will fit a linear regression model with x and x$^2$, in other words a polynomial of the 2nd degree. The sklearn command 'Linear Regression' is not particularly user friendly in terms of its output so we will summarise the results in a pandas data frame. We will look at the **training mean squared error, which is the average squared distance between the predicted y's from the model and the actual y's**. We will also look at the regression coefficients  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['x','x2']\n",
    "linreg = LinearRegression(fit_intercept=True,normalize=False)\n",
    "linreg.fit(data[predictors],data['y'])\n",
    "y_pred = linreg.predict(data[predictors])\n",
    "\n",
    "#code for plot\n",
    "plt.plot(data['x'],y_pred, label='OLS fit')\n",
    "plt.plot(data['x'],data['y'],'o',label='data')\n",
    "plt.plot(xg,f,label='true process')\n",
    "plt.title('Plot for power: 2')\n",
    "plt.legend()\n",
    "\n",
    "#output to return\n",
    "mse = np.sqrt(np.mean((y_pred-data['y'])**2))\n",
    "results = [mse]\n",
    "results.extend([linreg.intercept_])\n",
    "results.extend(linreg.coef_) \n",
    "results = pd.DataFrame([results],columns = ['MSE','intercept','x','x2'])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1\n",
    "\n",
    "Fit polynomials of 3rd and 9th degrees to the previous data and provide their coefficients and their training mean squared error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now introduce a more dense version of x and its powers (based on the array xg) in order to compare the model fit with the true process in the entire interval of $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg0 = np.ones(100)\n",
    "grid = pd.DataFrame(np.column_stack([xg0,xg]),columns=['x0','x'])\n",
    "for i in range(2,10):  \n",
    "    colname = 'x%d'%i \n",
    "    grid[colname] = grid['x']**i \n",
    "grid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code, in addition to fitting the polynomial on the data, also provides the fit that correspond to it in the entire interval $[0,1]$.\n",
    "\n",
    "Below we see the case for a 9-th degree polynomial but you can repeat for eny order between 1 and 9 by setting 'npower' to the desired order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npower=9\n",
    "grid_predictors = ['x0','x']\n",
    "grid_predictors.extend(['x%d'%i for i in range(2,npower+1)])\n",
    "X = grid[grid_predictors]\n",
    "predictors = ['x']\n",
    "predictors.extend(['x%d'%i for i in range(2,npower+1)])\n",
    "linreg = LinearRegression(fit_intercept=True,normalize=False)\n",
    "linreg.fit(data[predictors],data['y'])\n",
    "beta = [linreg.intercept_]\n",
    "beta.extend(linreg.coef_)\n",
    "fitted = np.dot(X,beta)\n",
    "\n",
    "plt.plot(grid['x'],fitted, label='OLS fit')\n",
    "plt.plot(data['x'],data['y'],'o',label='data')\n",
    "plt.plot(xg,f,label='true process')\n",
    "plt.title('Plot for power: %d'%npower)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "We now turn to Ridge regression. We will use all powers of x up to 9 but we also include a penalty term on the squared value of the coefficients. The amount penalisation is controlled by the parameter 'lam'.\n",
    "\n",
    "Below is some code that fits a Ridge regression model and provides (as before) the estimates of the coefficients and the training MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npower = 9\n",
    "predictors = ['x']\n",
    "predictors.extend(['x%d'%i for i in range(2,npower+1)])\n",
    "lam=np.exp(-10)\n",
    "ridgereg = Ridge(alpha=lam,normalize=False,fit_intercept=True)\n",
    "ridgereg.fit(data[predictors],data['y'])\n",
    "y_pred = ridgereg.predict(data[predictors])\n",
    "\n",
    "#plot\n",
    "plt.plot(data['x'],y_pred, label='OLS fit')\n",
    "plt.plot(data['x'],data['y'],'o',label='data')\n",
    "plt.plot(xg,f,label='true process')\n",
    "plt.title('Plot for ridge regression')\n",
    "plt.legend()\n",
    "\n",
    "#output to return\n",
    "mse = np.sqrt(np.mean((y_pred-data['y'])**2))\n",
    "results = [mse]\n",
    "results.extend([ridgereg.intercept_])\n",
    "results.extend(ridgereg.coef_)\n",
    "col = ['MSE','intercept'] + ['x%d'%i for i in range(1,npower+1)]\n",
    "results = pd.DataFrame([results],columns = col)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Error - Out of sample performance\n",
    "\n",
    "In the previous exercise we compared visually the fit from the model when applied to x being in the interval $[0,1]$ versus the true function. This is very useful but cannot be done in practice as the true function is unknown.\n",
    "\n",
    "We will therefore repeat the exercise under a more realistic setting: \n",
    "- Generate more 100 noisly observations from the function $sin(2\\pi x)$ as before\n",
    "- **Randomly select 10** of those. These will form the **train sample** that will be used to fit the models and obtain the regression coefficients. \n",
    "- Set **aside** the remaining 90 observations. These will for the **test sample** that will be used to assess the predictions of each model that was estimated in the train sample. \n",
    "- Contrasting those predictions to the real data will allow us to estimate the **test error** and assess the out of sample performance of each model.\n",
    "- Monitor both the training and test errors for each model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by simulate 100 noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate more data\n",
    "np.random.seed(4)\n",
    "x = np.linspace(0,1,100)\n",
    "f = np.sin(2*np.pi*x)\n",
    "y = np.sin(2*np.pi*x)+0.3*np.random.randn(100)\n",
    "plt.plot(x,f,label='true process')\n",
    "plt.plot(x,y,'o',label='data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(np.column_stack([y,x]),columns=['y','x']) #columns, then column names\n",
    "npower = 9\n",
    "for i in range(2,10):  \n",
    "    colname = 'x%d'%i \n",
    "    data[colname] = data['x']**i \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we randomly split the data to train and test samples and see those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npower = 9  #feel free to try different numbers for npower\n",
    "predictors = ['x']\n",
    "predictors.extend(['x%d'%i for i in range(2,npower+1)])\n",
    "\n",
    "# Split up your data\n",
    "trainX, testX, trainy, testy = train_test_split(data[predictors],data['y'],\n",
    "                                                test_size=0.9, random_state=1)\n",
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will \n",
    " 1. fit the ridge regession model with 9-th order polynomial on the train dataset\n",
    " 2. calculate the train error as before\n",
    " 2. obtain its predictions for the test dataset\n",
    " 3. compare the prediction of the previous step against the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npower = 9\n",
    "predictors = ['x']\n",
    "predictors.extend(['x%d'%i for i in range(2,npower+1)])\n",
    "\n",
    "#Step 1\n",
    "lam=np.exp(-2.5)\n",
    "ridgereg = Ridge(alpha=lam,normalize=False,fit_intercept=True)\n",
    "ridgereg.fit(trainX,trainy)\n",
    "\n",
    "#Step 2\n",
    "y_pred_train = ridgereg.predict(trainX)\n",
    "train_mse = np.sqrt(np.mean((y_pred_train-trainy)**2))\n",
    "\n",
    "#Steps 3 and 4\n",
    "y_pred_test = ridgereg.predict(testX)\n",
    "test_mse = np.sqrt(np.mean((y_pred_test-testy)**2))\n",
    "\n",
    "#output to return\n",
    "results = [train_mse,test_mse]\n",
    "results.extend([ridgereg.intercept_])\n",
    "results.extend(ridgereg.coef_)\n",
    "col = ['trainMSE','testMSE','intercept'] + ['x%d'%i for i in range(1,npower+1)]\n",
    "results = pd.DataFrame([results],columns = col)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for a linear regression model based on a polynomial of order 'npower' we will\n",
    " 1. fit the linear regession model of the 'npower'-th order polynomial on the train dataset\n",
    " 2. calculate the train error as before\n",
    " 2. obtain its predictions for the test dataset\n",
    " 3. compare the prediction of the previous step against the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "npower = 5\n",
    "for i in range(2,10):  \n",
    "    colname = 'x%d'%i \n",
    "    data[colname] = data['x']**i \n",
    "predictors = ['x']\n",
    "predictors.extend(['x%d'%i for i in range(2,npower+1)])\n",
    "linreg = LinearRegression(fit_intercept=True,normalize=False)\n",
    "linreg.fit(trainX[predictors],trainy)\n",
    "\n",
    "# Step 2\n",
    "y_pred_train = linreg.predict(trainX[predictors])\n",
    "train_mse = np.sqrt(np.mean((y_pred_train-trainy)**2))\n",
    "\n",
    "# Steps 3 and 4\n",
    "y_pred_test = linreg.predict(testX[predictors])\n",
    "test_mse = np.sqrt(np.mean((y_pred_test-testy)**2))\n",
    "\n",
    "# output to return\n",
    "results = [train_mse,test_mse]\n",
    "results.extend([linreg.intercept_])\n",
    "results.extend(linreg.coef_)\n",
    "col = ['trainMSE','testMSE','intercept'] + ['x%d'%i for i in range(1,npower+1)]\n",
    "results = pd.DataFrame([results],columns = col)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2\n",
    "\n",
    "1. Find the order of polynomial that produces the smallest test MSE. \n",
    "2. Can you beat that with Ridge Regression? In other words can you find penalty 'lam' such that the Ridge regression model produces an even smaller test MSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real data example\n",
    "\n",
    "Finally we will look at a real data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston() ## loads Boston dataset from datasets library\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "bos['PRICE'] = boston.target\n",
    "bos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3\n",
    "\n",
    "1. Split the data into train and test samples, such that the test size is 30\\%\n",
    "2. Fit a linear regression and a ridge regression model on the train data\n",
    "3. Obtain the predictions from both models on the test data and compare\n",
    "4. Repeat for a test size of 95\\%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to split into X and Y matrices/vectors first with the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bos.drop('PRICE', axis = 1)\n",
    "Y = bos['PRICE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
